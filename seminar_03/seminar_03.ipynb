{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Семинар 3: Бинарная сегментация для Луны\n",
        "\n",
        "В этом семинаре мы будем решать задачу **бинарной сегментации** - определять где на лунной поверхности находятся камни.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Создание кастомного Dataset для Moon Segmentation\n",
        "\n",
        "### Структура данных:\n",
        "- **render/** - исходные изображения лунной поверхности (1000 изображений)\n",
        "- **ground/** - бинарные маски сегментации (0 = фон, 255 = камни)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MoonSegmentationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, image_folder='render', mask_folder='ground', \n",
        "                 image_ids=None, augmentation=None, preprocessing=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_folder = image_folder\n",
        "        self.mask_folder = mask_folder\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "        images_dir = os.path.join(root_dir, 'images', image_folder)\n",
        "        masks_dir = os.path.join(root_dir, 'images', mask_folder)\n",
        "        \n",
        "        if image_ids is None:\n",
        "            all_images = os.listdir(images_dir)\n",
        "            self.image_ids = [img.replace('.png', '') for img in all_images if img.endswith('.png')]\n",
        "        else:\n",
        "            self.image_ids = image_ids\n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        \n",
        "        image_path = os.path.join(self.root_dir, 'images', self.image_folder, f\"{image_id}.png\")\n",
        "        \n",
        "        # Для масок убираем префикс \"render\" если он есть\n",
        "        # Например: render0001 - 0001\n",
        "        mask_id = image_id.replace('render', '') if 'render' in image_id else image_id\n",
        "        mask_path = os.path.join(self.root_dir, 'images', self.mask_folder, f\"ground{mask_id}.png\")\n",
        "        \n",
        "        image = cv2.imread(image_path)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        \n",
        "        # Так как используем opencv, то не забываем преводить из BGR в RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Нормализуем бинарную маску к [0, 1]\n",
        "        # 0 = фон, 1 = камни\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "        \n",
        "        # Применяем аугментации\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # Применяем предобработку\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Создание аугментаций с Albumentations\n",
        "\n",
        "Для задачи сегментации важно применять одинаковые трансформации к изображению и маске!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Аугментации для обучения\n",
        "train_augmentation = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "    A.OneOf([\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0)),\n",
        "        A.GaussianBlur(blur_limit=(3, 7)),\n",
        "        A.MedianBlur(blur_limit=5),\n",
        "    ], p=0.3),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "])\n",
        "\n",
        "preprocessing = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_augmentation = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Разделение данных на train/val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Путь к данным\n",
        "DATA_ROOT = \"data/MOON_SEGMENTATION_BINARY/\"\n",
        "\n",
        "images_dir = os.path.join(DATA_ROOT, 'images', 'render')\n",
        "all_images = [img.replace('.png', '') for img in os.listdir(images_dir) if img.endswith('.png')][:100]\n",
        "\n",
        "print(f\"Всего изображений: {len(all_images)}\")\n",
        "\n",
        "train_ids, val_ids = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(train_ids)} изображений\")\n",
        "print(f\"Val: {len(val_ids)} изображений\")\n",
        "\n",
        "train_dataset = MoonSegmentationDataset(\n",
        "    root_dir=DATA_ROOT,\n",
        "    image_folder='render',\n",
        "    mask_folder='ground',\n",
        "    image_ids=train_ids,\n",
        "    augmentation=train_augmentation,\n",
        "    preprocessing=preprocessing\n",
        ")\n",
        "\n",
        "val_dataset = MoonSegmentationDataset(\n",
        "    root_dir=DATA_ROOT,\n",
        "    image_folder='render',\n",
        "    mask_folder='ground',\n",
        "    image_ids=val_ids,\n",
        "    augmentation=val_augmentation,\n",
        "    preprocessing=preprocessing\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False, \n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Батчей в train: {len(train_loader)}\")\n",
        "print(f\"Батчей в val: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Визуализация данных\n",
        "\n",
        "Посмотрим на примеры изображений и их маски сегментации\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize(img_tensor):\n",
        "    \"\"\"Денормализация изображения для визуализации\"\"\"\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    img = img_tensor * std + mean\n",
        "    return img.clamp(0, 1)\n",
        "\n",
        "images, masks = next(iter(train_loader))\n",
        "\n",
        "print(f\"Размер батча изображений: {images.shape}\")\n",
        "print(f\"Размер батча масок: {masks.shape}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "for i in range(min(4, len(images))):\n",
        "    img = denormalize(images[i]).permute(1, 2, 0).numpy()\n",
        "    mask = masks[i].numpy()\n",
        "    \n",
        "    axes[0, i].imshow(img)\n",
        "    axes[0, i].set_title(f\"Изображение {i+1}\")\n",
        "    axes[0, i].axis('off')\n",
        "    \n",
        "    axes[1, i].imshow(mask, cmap='gray')\n",
        "    axes[1, i].set_title(f\"Маска {i+1} (белый=камни)\")\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.suptitle(\"Примеры данных: Бинарная сегментация камней на Луне\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Кастомная U-Net архитектура\n",
        "\n",
        "### Что такое U-Net?\n",
        "**U-Net** - это архитектура для семантической сегментации, предложенная в 2015 году.\n",
        "\n",
        "### Основные компоненты:\n",
        "1. **Encoder (Downsampling)** - сжимает изображение, извлекая признаки\n",
        "2. **Bottleneck** - самый глубокий слой с максимальным количеством каналов\n",
        "3. **Decoder (Upsampling)** - восстанавливает разрешение\n",
        "4. **Skip Connections** - соединяют encoder и decoder для сохранения деталей\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Полная кастомная U-Net архитектура\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNet, self).__init__()\n",
        "        \n",
        "        self.encoder_blocks = nn.ModuleList()\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        prev_channels = in_channels\n",
        "        for feature in features:\n",
        "            self.encoder_blocks.append(DoubleConv(prev_channels, feature))\n",
        "            prev_channels = feature\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "\n",
        "        for feature in reversed(features):\n",
        "            self.decoder_blocks.append(\n",
        "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.decoder_blocks.append(\n",
        "                DoubleConv(feature * 2, feature)\n",
        "            )\n",
        "\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "        \n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Параметров: {total_params:,}\")\n",
        "        print(f\"Уровней encoder: {len(features)}\")\n",
        "        print(f\"Конфигурация каналов: {features}\")\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        skip_connections = []\n",
        "\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "        \n",
        "        x = self.bottleneck(x)\n",
        "        \n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.decoder_blocks), 2):\n",
        "            x = self.decoder_blocks[idx](x)\n",
        "            \n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "            \n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:], \n",
        "                                mode='bilinear', align_corners=True)\n",
        "            \n",
        "            x = torch.cat([skip_connection, x], dim=1)\n",
        "            \n",
        "            x = self.decoder_blocks[idx + 1](x)\n",
        "\n",
        "        output = self.final_conv(x)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "model = UNet(in_channels=3, out_channels=1, features=[64, 128, 256, 512])\n",
        "\n",
        "test_input = torch.randn(1, 3, 256, 256)\n",
        "test_output = model(test_input)\n",
        "\n",
        "print(f\"   Вход:  {test_input.shape}\")\n",
        "print(f\"   Выход: {test_output.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Функции потерь и метрики для сегментации\n",
        "\n",
        "### Метрики сегментации:\n",
        "- **Dice Loss** - популярная функция потерь для сегментации\n",
        "- **IoU (Intersection over Union)** - метрика качества сегментации\n",
        "- **Pixel Accuracy** - точность предсказания пикселей\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss для бинарной сегментации\n",
        "    \n",
        "    Dice = 2 * |X ∩ Y| / (|X| + |Y|)\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "    \n",
        "    def forward(self, predictions, targets):\n",
        "        predictions = torch.sigmoid(predictions)\n",
        "        \n",
        "        # Flatten\n",
        "        predictions = predictions.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        # Dice coefficient\n",
        "        intersection = (predictions * targets).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
        "        \n",
        "        # Dice loss\n",
        "        return 1 - dice\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.dice = DiceLoss()\n",
        "    \n",
        "    def forward(self, predictions, targets):\n",
        "        bce_loss = self.bce(predictions, targets)\n",
        "        dice_loss = self.dice(predictions, targets)\n",
        "        \n",
        "        return self.bce_weight * bce_loss + self.dice_weight * dice_loss\n",
        "\n",
        "\n",
        "def dice_coefficient(predictions, targets, threshold=0.5, smooth=1e-6):\n",
        "\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "    predictions = (predictions > threshold).float()\n",
        "    \n",
        "    predictions = predictions.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "    \n",
        "    intersection = (predictions * targets).sum()\n",
        "    dice = (2. * intersection + smooth) / (predictions.sum() + targets.sum() + smooth)\n",
        "    \n",
        "    return dice.item()\n",
        "\n",
        "\n",
        "def iou_score(predictions, targets, threshold=0.5, smooth=1e-6):\n",
        "\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "    predictions = (predictions > threshold).float()\n",
        "    \n",
        "    predictions = predictions.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "    \n",
        "    intersection = (predictions * targets).sum()\n",
        "    union = predictions.sum() + targets.sum() - intersection\n",
        "    \n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    \n",
        "    return iou.item()\n",
        "\n",
        "\n",
        "def pixel_accuracy(predictions, targets, threshold=0.5):\n",
        "\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "    predictions = (predictions > threshold).float()\n",
        "    correct = (predictions == targets).float().sum()\n",
        "    total = targets.numel()\n",
        "    \n",
        "    return (correct / total).item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    running_dice = 0.0\n",
        "    running_iou = 0.0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    \n",
        "    for images, masks in pbar:\n",
        "        images = images.to(device)\n",
        "        masks = masks.unsqueeze(1).to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, masks)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            dice = dice_coefficient(outputs, masks)\n",
        "            iou = iou_score(outputs, masks)\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        running_dice += dice\n",
        "        running_iou += iou\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'dice': f'{dice:.4f}',\n",
        "            'iou': f'{iou:.4f}'\n",
        "        })\n",
        "    \n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    avg_dice = running_dice / len(train_loader)\n",
        "    avg_iou = running_iou / len(train_loader)\n",
        "    \n",
        "    return avg_loss, avg_dice, avg_iou\n",
        "\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    running_dice = 0.0\n",
        "    running_iou = 0.0\n",
        "    running_acc = 0.0\n",
        "    \n",
        "    pbar = tqdm(val_loader, desc=\"Validation\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, masks in pbar:\n",
        "            images = images.to(device)\n",
        "            masks = masks.unsqueeze(1).to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            \n",
        "            loss = criterion(outputs, masks)\n",
        "            \n",
        "            dice = dice_coefficient(outputs, masks)\n",
        "            iou = iou_score(outputs, masks)\n",
        "            acc = pixel_accuracy(outputs, masks)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            running_dice += dice\n",
        "            running_iou += iou\n",
        "            running_acc += acc\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'dice': f'{dice:.4f}',\n",
        "                'iou': f'{iou:.4f}',\n",
        "                'acc': f'{acc:.4f}'\n",
        "            })\n",
        "    \n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    avg_dice = running_dice / len(val_loader)\n",
        "    avg_iou = running_iou / len(val_loader)\n",
        "    avg_acc = running_acc / len(val_loader)\n",
        "    \n",
        "    return avg_loss, avg_dice, avg_iou, avg_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Основной цикл обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=20, save_path='best_unet.pth'):\n",
        "\n",
        "    model = model.to(device)\n",
        "    \n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_dice': [],\n",
        "        'train_iou': [],\n",
        "        'val_loss': [],\n",
        "        'val_dice': [],\n",
        "        'val_iou': [],\n",
        "        'val_acc': [],\n",
        "        'lr': []\n",
        "    }\n",
        "    \n",
        "    best_val_dice = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Эпоха {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        # Здесь происходит основное обучение\n",
        "        train_loss, train_dice, train_iou = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        \n",
        "        # Валидация \n",
        "        val_loss, val_dice, val_iou, val_acc = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Обновляем learning rate (проверяем на )\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_dice)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "        else:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Сохраняем всю нашу историю\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_dice'].append(train_dice)\n",
        "        history['train_iou'].append(train_iou)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_dice'].append(val_dice)\n",
        "        history['val_iou'].append(val_iou)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['lr'].append(current_lr)\n",
        "        \n",
        "        # Выводим результаты\n",
        "        print(f\"\\n Результаты эпохи {epoch+1}:\")\n",
        "        print(f\"  Train - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}, IoU: {train_iou:.4f}\")\n",
        "        print(f\"  Val   - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}, IoU: {val_iou:.4f}, Acc: {val_acc:.4f}\")\n",
        "        print(f\"  LR: {current_lr:.6f}\")\n",
        "        \n",
        "        # Сохраняем лучшую модел, можно сохранять по метрике или loss\n",
        "        if val_dice > best_val_dice:\n",
        "            best_val_dice = val_dice\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_dice': val_dice,\n",
        "                'val_iou': val_iou,\n",
        "            }, save_path)\n",
        "            print(f\"  Сохранена лучшая модель! Dice: {val_dice:.4f}\")\n",
        "    \n",
        "    print(f\"Лучший Val Dice: {best_val_dice:.4f}\") \n",
        "    \n",
        "    return history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Запуск обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Настройка обучения\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"  Используем устройство: {device}\")\n",
        "\n",
        "model = UNet(in_channels=3, out_channels=1, features=[32, 64, 128, 256])\n",
        "\n",
        "# Функция потерь (комбинированная BCE + Dice)\n",
        "criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Планировщик learning rate (уменьшаем lr при плато)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    mode='max',\n",
        "    factor=0.5, \n",
        "    patience=3, \n",
        ")\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    save_path='best_moon_unet.pth'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Визуализация результатов обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='o')\n",
        "    axes[0, 0].set_title('Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Dice Coefficient\n",
        "    axes[0, 1].plot(history['train_dice'], label='Train Dice', marker='o')\n",
        "    axes[0, 1].plot(history['val_dice'], label='Val Dice', marker='o')\n",
        "    axes[0, 1].set_title('Dice Coefficient', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Dice')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # IoU Score\n",
        "    axes[1, 0].plot(history['train_iou'], label='Train IoU', marker='o')\n",
        "    axes[1, 0].plot(history['val_iou'], label='Val IoU', marker='o')\n",
        "    axes[1, 0].set_title('IoU Score', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('IoU')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Learning Rate\n",
        "    axes[1, 1].plot(history['lr'], label='Learning Rate', marker='o', color='green')\n",
        "    axes[1, 1].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('LR')\n",
        "    axes[1, 1].set_yscale('log')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('История обучения U-Net', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    print(f\"Train Loss: {history['train_loss'][-1]:.4f}\")\n",
        "    print(f\"Val Loss: {history['val_loss'][-1]:.4f}\")\n",
        "    print(f\"Train Dice: {history['train_dice'][-1]:.4f}\")\n",
        "    print(f\"Val Dice: {history['val_dice'][-1]:.4f}\")\n",
        "    print(f\"Train IoU: {history['train_iou'][-1]:.4f}\")\n",
        "    print(f\"Val IoU: {history['val_iou'][-1]:.4f}\")\n",
        "    print(f\"Val Accuracy: {history['val_acc'][-1]:.4f}\")\n",
        "\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Визуализация предсказаний модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataloader, device, num_samples=4):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    images, masks = next(iter(dataloader))\n",
        "    images = images.to(device)\n",
        "    masks = masks.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        predictions = torch.sigmoid(outputs)\n",
        "        predictions = (predictions > 0.5).float()\n",
        "    \n",
        "    images = images.cpu()\n",
        "    masks = masks.cpu()\n",
        "    predictions = predictions.cpu()\n",
        "    \n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "    \n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        img = denormalize(images[i]).permute(1, 2, 0).numpy()\n",
        "        mask_true = masks[i, :,:].numpy()\n",
        "        mask_pred = predictions[i, 0].numpy()\n",
        "        \n",
        "        axes[i, 0].imshow(img)\n",
        "        axes[i, 0].set_title('Исходное изображение', fontsize=12)\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        axes[i, 1].imshow(mask_true, cmap='gray')\n",
        "        axes[i, 1].set_title('Истинная маска', fontsize=12)\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        axes[i, 2].imshow(mask_pred, cmap='gray')\n",
        "        axes[i, 2].set_title('Предсказание', fontsize=12)\n",
        "        axes[i, 2].axis('off')\n",
        "        \n",
        "        overlay = img.copy()\n",
        "        overlay[mask_pred > 0.5] = [0, 1, 0]\n",
        "        \n",
        "        axes[i, 3].imshow(overlay)\n",
        "        axes[i, 3].set_title('Наложение', fontsize=12)\n",
        "        axes[i, 3].axis('off')\n",
        "        \n",
        "        dice = dice_coefficient(outputs[i:i+1], masks[i:i+1])\n",
        "        iou = iou_score(outputs[i:i+1], masks[i:i+1])\n",
        "        \n",
        "        fig.text(0.5, 1 - (i + 0.5) / num_samples, \n",
        "                f'Dice: {dice:.4f} | IoU: {iou:.4f}',\n",
        "                ha='center', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('Предсказания модели U-Net', fontsize=16, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(model, val_loader, device, num_samples=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Загрузка и тестирование лучшей модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = UNet(in_channels=3, out_channels=1, features=[32, 64, 128, 256])\n",
        "\n",
        "checkpoint = torch.load('best_moon_unet.pth', map_location=device)\n",
        "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "print(f\"Эпоха: {checkpoint['epoch'] + 1}\")\n",
        "print(f\"Val Dice: {checkpoint['val_dice']:.4f}\")\n",
        "print(f\"Val IoU: {checkpoint['val_iou']:.4f}\")\n",
        "\n",
        "visualize_predictions(best_model, val_loader, device, num_samples=6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_image(model, image_path, device, threshold=0.5):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    original_image = image.copy()\n",
        "    \n",
        "    augmented = val_augmentation(image=image)\n",
        "    image = augmented['image']\n",
        "    preprocessed = preprocessing(image=image)\n",
        "    image_tensor = preprocessed['image'].unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        prediction = torch.sigmoid(output)\n",
        "        prediction = (prediction > threshold).float()\n",
        "    \n",
        "    prediction = prediction.cpu().squeeze().numpy()\n",
        "    \n",
        "    return prediction, original_image\n",
        "\n",
        "\n",
        "def visualize_single_prediction(image_path, prediction, original_image):\n",
        "\n",
        "    overlay = cv2.resize(original_image, (prediction.shape[1], prediction.shape[0]))\n",
        "    overlay = overlay.astype(float) / 255.0\n",
        "    overlay[:, :, 1] = np.where(prediction > 0.5, 1, overlay[:, :, 1])\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    axes[0].imshow(original_image)\n",
        "    axes[0].set_title('Исходное изображение', fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(prediction, cmap='gray')\n",
        "    axes[1].set_title('Предсказание модели', fontsize=12, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title('Наложение', fontsize=12, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'Инференс: {os.path.basename(image_path)}', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "random_image_id = np.random.choice(val_ids)\n",
        "image_path = os.path.join(DATA_ROOT, 'images', 'render', f'{random_image_id}.png')\n",
        "\n",
        "prediction, original_image = predict_image(best_model, image_path, device)\n",
        "visualize_single_prediction(image_path, prediction, original_image)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Часть 2: U-Net с ResNet18 Backbone\n",
        "\n",
        "Теперь создадим более мощную версию U-Net, используя предобученную ResNet18 как энкодер (backbone).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class ResNet18UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, out_channels=1, pretrained=True):\n",
        "        super(ResNet18UNet, self).__init__()\n",
        "\n",
        "        # Загружаем предобученную ResNet18\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        \n",
        "        # Извлекаем слои энкодера\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            resnet.conv1,      # 64 каналов, stride=2\n",
        "            resnet.bn1,\n",
        "            resnet.relu\n",
        "        )\n",
        "        self.pool1 = resnet.maxpool  # stride=2\n",
        "        \n",
        "        self.encoder2 = resnet.layer1  # 64 каналов\n",
        "        self.encoder3 = resnet.layer2  # 128 каналов, stride=2\n",
        "        self.encoder4 = resnet.layer3  # 256 каналов, stride=2\n",
        "        self.encoder5 = resnet.layer4  # 512 каналов, stride=2\n",
        "        \n",
        "        # Decoder блок 1 (512 -> 256)\n",
        "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder4 = DoubleConv(256 + 256, 256)  # Concat с encoder4\n",
        "        \n",
        "        # Decoder блок 2 (256 -> 128)\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder3 = DoubleConv(128 + 128, 128)  # Concat с encoder3\n",
        "        \n",
        "        # Decoder блок 3 (128 -> 64)\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder2 = DoubleConv(64 + 64, 64)  # Concat с encoder2\n",
        "        \n",
        "        # Decoder блок 4 (64 -> 64)\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = DoubleConv(64 + 64, 64)  # Concat с encoder1\n",
        "        \n",
        "        # Финальный upsampling до исходного размера\n",
        "        self.final_upconv = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        \n",
        "        # Выходной слой\n",
        "        self.out_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        # Исходный размер: [B, 3, H, W]\n",
        "        \n",
        "        enc1 = self.encoder1(x)      # [B, 64, H/2, W/2]\n",
        "        enc1_pooled = self.pool1(enc1)  # [B, 64, H/4, W/4]\n",
        "        \n",
        "        enc2 = self.encoder2(enc1_pooled)  # [B, 64, H/4, W/4]\n",
        "        enc3 = self.encoder3(enc2)         # [B, 128, H/8, W/8]\n",
        "        enc4 = self.encoder4(enc3)         # [B, 256, H/16, W/16]\n",
        "        enc5 = self.encoder5(enc4)         # [B, 512, H/32, W/32]\n",
        "        \n",
        "        # Decoder блок 1\n",
        "        dec4 = self.upconv4(enc5)           # [B, 256, H/16, W/16]\n",
        "        dec4 = torch.cat([dec4, enc4], dim=1)  # [B, 512, H/16, W/16]\n",
        "        dec4 = self.decoder4(dec4)          # [B, 256, H/16, W/16]\n",
        "        \n",
        "        # Decoder блок 2\n",
        "        dec3 = self.upconv3(dec4)           # [B, 128, H/8, W/8]\n",
        "        dec3 = torch.cat([dec3, enc3], dim=1)  # [B, 256, H/8, W/8]\n",
        "        dec3 = self.decoder3(dec3)          # [B, 128, H/8, W/8]\n",
        "        \n",
        "        # Decoder блок 3\n",
        "        dec2 = self.upconv2(dec3)           # [B, 64, H/4, W/4]\n",
        "        dec2 = torch.cat([dec2, enc2], dim=1)  # [B, 128, H/4, W/4]\n",
        "        dec2 = self.decoder2(dec2)          # [B, 64, H/4, W/4]\n",
        "        \n",
        "        # Decoder блок 4\n",
        "        dec1 = self.upconv1(dec2)           # [B, 64, H/2, W/2]\n",
        "        dec1 = torch.cat([dec1, enc1], dim=1)  # [B, 128, H/2, W/2]\n",
        "        dec1 = self.decoder1(dec1)          # [B, 64, H/2, W/2]\n",
        "        \n",
        "        # Финальный upsampling\n",
        "        final = self.final_upconv(dec1)     # [B, 32, H, W]\n",
        "        \n",
        "        # Выходной слой\n",
        "        out = self.out_conv(final)          # [B, 1, H, W]\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "resnet_unet = ResNet18UNet(out_channels=1, pretrained=True)\n",
        "\n",
        "test_input = torch.randn(1, 3, 256, 256)\n",
        "test_output = resnet_unet(test_input)\n",
        "\n",
        "print(f\"   Вход:  {test_input.shape}\")\n",
        "print(f\"   Выход: {test_output.shape}\")\n",
        "\n",
        "total_params = sum(p.numel() for p in resnet_unet.parameters())\n",
        "trainable_params = sum(p.numel() for p in resnet_unet.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"   Всего параметров: {total_params:,}\")\n",
        "print(f\"   Обучаемых параметров: {trainable_params:,}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Обучение ResNet18-UNet\n",
        "\n",
        "Теперь обучим новую модель на том же датасете и сравним результаты с простой U-Net.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_model = ResNet18UNet(out_channels=1, pretrained=True)\n",
        "\n",
        "resnet_criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
        "\n",
        "# Разделяем параметры на encoder (ResNet) и decoder\n",
        "encoder_params = []\n",
        "decoder_params = []\n",
        "\n",
        "for name, param in resnet_model.named_parameters():\n",
        "    if 'encoder' in name:\n",
        "        encoder_params.append(param)\n",
        "    else:\n",
        "        decoder_params.append(param)\n",
        "\n",
        "# Encoder обучаем с меньшим LR (fine-tuning), decoder - с обычным\n",
        "resnet_optimizer = torch.optim.Adam([\n",
        "    {'params': encoder_params, 'lr': 1e-4},  # Меньший LR для предобученных слоев\n",
        "    {'params': decoder_params, 'lr': 1e-3}   # Обычный LR для новых слоев\n",
        "])\n",
        "\n",
        "resnet_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    resnet_optimizer, \n",
        "    mode='min', \n",
        "    factor=0.5, \n",
        "    patience=5, \n",
        ")\n",
        "\n",
        "NUM_EPOCHS_RESNET = 10\n",
        "SAVE_PATH_RESNET = 'best_resnet18_unet.pth'\n",
        "\n",
        "print(f\"   Эпох: {NUM_EPOCHS_RESNET}\")\n",
        "print(f\"   Encoder LR: 1e-4 (fine-tuning)\")\n",
        "print(f\"   Decoder LR: 1e-3\")\n",
        "print(f\"   Модель будет сохранена в: {SAVE_PATH_RESNET}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_history = train_model(\n",
        "    model=resnet_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=resnet_criterion,\n",
        "    optimizer=resnet_optimizer,\n",
        "    scheduler=resnet_scheduler,\n",
        "    num_epochs=NUM_EPOCHS_RESNET,\n",
        "    device=device,\n",
        "    save_path=SAVE_PATH_RESNET\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Визуализация результатов ResNet18-UNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_training_history(resnet_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_resnet_model = ResNet18UNet(out_channels=1, pretrained=False)\n",
        "\n",
        "checkpoint = torch.load(SAVE_PATH_RESNET, map_location=device)\n",
        "best_resnet_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "best_resnet_model = best_resnet_model.to(device)\n",
        "\n",
        "print(f\"   Best epoch: {checkpoint['epoch']}\")\n",
        "print(f\"   Best val Dice: {checkpoint['val_dice']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "visualize_predictions(best_resnet_model, val_loader, device, num_samples=4)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Сравнение моделей: U-Net vs ResNet18-UNet\n",
        "\n",
        "Давайте сравним результаты обеих моделей на одних и тех же изображениях.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_models(simple_model, resnet_model, dataloader, device, num_samples=3):\n",
        "\n",
        "    simple_model.eval()\n",
        "    resnet_model.eval()\n",
        "    \n",
        "    images, masks = next(iter(dataloader))\n",
        "    images = images.to(device)\n",
        "    masks = masks.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        simple_outputs = simple_model(images)\n",
        "        simple_preds = torch.sigmoid(simple_outputs)\n",
        "        simple_preds = (simple_preds > 0.5).float()\n",
        "        \n",
        "        resnet_outputs = resnet_model(images)\n",
        "        resnet_preds = torch.sigmoid(resnet_outputs)\n",
        "        resnet_preds = (resnet_preds > 0.5).float()\n",
        "    \n",
        "    images = images.cpu()\n",
        "    masks = masks.cpu()\n",
        "    simple_preds = simple_preds.cpu()\n",
        "    resnet_preds = resnet_preds.cpu()\n",
        "    \n",
        "    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
        "    \n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        # Денормализуем изображение\n",
        "        img = denormalize(images[i]).permute(1, 2, 0).numpy()\n",
        "        mask_true = masks[i, 0].numpy()\n",
        "        simple_pred = simple_preds[i, 0].numpy()\n",
        "        resnet_pred = resnet_preds[i, 0].numpy()\n",
        "        \n",
        "        axes[i, 0].imshow(img)\n",
        "        axes[i, 0].set_title('Исходное', fontsize=12)\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        axes[i, 1].imshow(mask_true, cmap='gray')\n",
        "        axes[i, 1].set_title('Ground Truth', fontsize=12)\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        axes[i, 2].imshow(simple_pred, cmap='gray')\n",
        "        simple_dice = dice_coefficient(simple_outputs[i:i+1], masks[i:i+1])\n",
        "        simple_iou = iou_score(simple_outputs[i:i+1], masks[i:i+1])\n",
        "        axes[i, 2].set_title(f'Simple U-Net\\nDice: {simple_dice:.3f} | IoU: {simple_iou:.3f}', fontsize=10)\n",
        "        axes[i, 2].axis('off')\n",
        "        \n",
        "        axes[i, 3].imshow(resnet_pred, cmap='gray')\n",
        "        resnet_dice = dice_coefficient(resnet_outputs[i:i+1], masks[i:i+1])\n",
        "        resnet_iou = iou_score(resnet_outputs[i:i+1], masks[i:i+1])\n",
        "        axes[i, 3].set_title(f'ResNet18-UNet\\nDice: {resnet_dice:.3f} | IoU: {resnet_iou:.3f}', fontsize=10)\n",
        "        axes[i, 3].axis('off')\n",
        "        \n",
        "        diff = np.abs(simple_pred - resnet_pred)\n",
        "        axes[i, 4].imshow(diff, cmap='hot')\n",
        "        axes[i, 4].set_title('Difference\\n(red = disagree)', fontsize=10)\n",
        "        axes[i, 4].axis('off')\n",
        "    \n",
        "    plt.suptitle('Сравнение моделей: U-Net vs ResNet18-UNet', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_models(best_model, best_resnet_model, val_loader, device, num_samples=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
