# Домашнее задание 2: Создание и оптимизация ResNet18

---

## Общая цель

Поэтапная разработка кастомной ResNet18 модели для классификации Tiny ImageNet с анализом влияния различных архитектурных решений на производительность.

**Датасет:** Tiny ImageNet (200 классов) - выберите **10 классов** самостоятельно для работы

---

## Часть 1: Подготовка данных

### Создание датакласса

Реализуйте собственный класс `TinyImageNetDataset`, наследующий от `torch.utils.data.Dataset`:

- Метод `__init__`: инициализация путей к данным, загрузка списка изображений и меток
- Метод `__len__`: возврат количества примеров в датасете
- Метод `__getitem__`: загрузка и возврат одного примера (изображение + метка)

## Часть 2: Базовая архитектура ResNet18

### 2.1. Реализация Basic Block

Создайте базовый residual блок со следующей структурой:

```
Input
  ↓
Conv2d(kernel_size=3, padding=1, stride=stride)
  ↓
BatchNorm2d
  ↓
ReLU
  ↓
Conv2d(kernel_size=3, padding=1, stride=1)
  ↓
BatchNorm2d
  ↓
  + ← Skip Connection (с возможностью downsample)
  ↓
ReLU
  ↓
Output
```

**Важно:**
- Если входные и выходные размерности не совпадают, используйте skip connection с Conv2d(1x1) + BatchNorm2d
- Первый residual блок может иметь stride=2 для уменьшения пространственного размера

### 2.2. Реализация ResNet18

Создайте архитектуру ResNet18 со следующей структурой:

```
Input (3, 64, 64)
  ↓
Conv2d(3→64, kernel_size=7, stride=2, padding=3)  # или 3x3, stride=1 для Tiny ImageNet
  ↓
BatchNorm2d
  ↓
ReLU
  ↓
MaxPool2d(kernel_size=3, stride=2, padding=1)  # опционально для Tiny ImageNet
  ↓
Layer1: 2x Basic Block (64 channels)
  ↓
Layer2: 2x Basic Block (128 channels, stride=2 в первом блоке)
  ↓
Layer3: 2x Basic Block (256 channels, stride=2 в первом блоке)
  ↓
Layer4: 2x Basic Block (512 channels, stride=2 в первом блоке)  # ОПЦИОНАЛЬНО
  ↓
AdaptiveAvgPool2d(output_size=(1, 1))
  ↓
Flatten
  ↓
Linear(512 → 10)  # 10 классов
  ↓
Output
```

### 2.3. Ограничения для базовой модели:

- **Общее количество параметров:** не более **5 миллионов**
- **Максимальное количество каналов:** до **512**

### 2.4. Скрипт обучения

Реализуйте цикл обучения с следующими компонентами:

**Оптимизатор:**
- Adam или SGD
- Learning rate: 0.001

**Loss function:**
- CrossEntropyLoss

**Метрики:**
- Accuracy для train/validation
- Loss для train/validation

**Обучение:**
- Количество эпох: 20-30
- Логирование метрик на каждой эпохе

### 2.5: Визуализация базовых результатов

После обучения базовой модели создайте:

1. **График Accuracy:**
   - X: эпохи
   - Y: accuracy
   - Две линии: train и validation

2. **График Loss:**
   - X: эпохи
   - Y: loss
   - Две линии: train и validation

3. **Информация о модели:**
   - Общее количество параметров
   - Архитектура (выведите через print(model))


## Часть 3: Поэтапная оптимизация модели

### 3.1: Оптимизация количества каналов

**Цель:** Изучение влияния количества каналов на производительность.

**Эксперимент:**
- Создайте 2 варианта модели:
  - **Вариант A:** 32 → 64 → 128 → 256 каналов
  - **Вариант B:** 64 → 128 → 256 каналов (без 4-го слоя)
- Обучите обе модели с теми же гиперпараметрами
- Сравните:
  - Количество параметров
  - Validation accuracy

**Результат:**
- Таблица сравнения
- Графики accuracy и loss для обоих вариантов
- Вывод: какая конфигурация лучше?

### 3.2: Эксперименты с количеством residual блоков

**Цель:** Изучение влияния глубины сети (количества residual блоков) на производительность.

**Эксперимент:**
- Используйте лучшую конфигурацию каналов из Этапа 3.1
- Создайте 3 варианта модели с разным количеством блоков в каждом слое:
  - **Вариант A:** [1, 1, 1, 1] - по 1 блоку в каждом слое (мелкая сеть)
  - **Вариант B:** [2, 2, 2, 2] - по 2 блока в каждом слое (стандартная ResNet18)
  - **Вариант C:** [3, 3, 3, 3] - по 3 блока в каждом слое (глубокая сеть)
- Обучите все три модели с одинаковыми гиперпараметрами

**Архитектура вариантов:**
```
Вариант A (4 блока):  Layer1[1 блок] → Layer2[1 блок] → Layer3[1 блок] → Layer4[1 блок]
Вариант B (8 блоков): Layer1[2 блока] → Layer2[2 блока] → Layer3[2 блока] → Layer4[2 блока]
Вариант C (12 блоков): Layer1[3 блока] → Layer2[3 блока] → Layer3[3 блока] → Layer4[3 блока]
```

**Результат:**
- Сравните:
  - Количество параметров
  - Validation accuracy
  - Скорость сходимости (на каких эпохах модель достигает лучших результатов)
- Графики accuracy для всех трех вариантов на одном графике
- Анализ: какая глубина оптимальна? Есть ли переобучение у более глубоких моделей?
- Вывод: какое количество блоков работает лучше?

### 3.3: Эксперименты с функциями активации

**Цель:** Исследование влияния различных активаций на обучение.

**Модификация модели:**
Замените ReLU на другие функции активации:

**Эксперимент:**
- Используйте лучшую конфигурацию из Этапа 3.2 (каналы + количество блоков)
- Обучите модели с разными активациями:
  - **Вариант A:** ReLU (baseline)
  - **Вариант B:** LeakyReLU
  - **Вариант C:** ELU
  - **Вариант D:** GELU

**Важно:** Используйте `inplace=True` где возможно для экономии памяти

**Результат:**
- Сравнение скорости сходимости (accuracy на каждой эпохе)
- Финальная validation accuracy
- Вывод: какая активация работает лучше?


## Часть 4: Финальная модель и тестирование

### 4.1: Создание финальной модели

На основе всех экспериментов:
- Выберите лучшую конфигурацию каналов (из Этапа 3.1)
- Выберите оптимальное количество residual блоков (из Этапа 3.2) - объясните, почему выбрали именно это количество
- Выберите лучшую функцию активации (из Этапа 3.3)
- Обучите финальную модель на 30-40 эпох с выбранными параметрами

**Дополнительно (опционально):**
- Добавьте data augmentation
- Попробуйте другие оптимизаторы (AdamW как вариант)

### 4.2: Тестирование на test set

После обучения финальной модели:

1. **Загрузите лучшую модель** (сохраненную по validation accuracy)
2. **Оцените на test set:**
   - Accuracy
   - Precision, Recall, F1-score для каждого класса
   - Confusion Matrix

### 4.3: Визуальный анализ

Создайте визуализацию с 10 случайными примерами из test set:

```
[Изображение 1] | Истинный класс: cat    | Предсказание: cat
[Изображение 2] | Истинный класс: dog    | Предсказание: wolf
...
```

### 4.4: Сравнительная таблица всех экспериментов

Создайте итоговую таблицу со всеми результатами:

| Этап | Конфигурация | Параметры | Val Accuracy | Train Accuracy |
|------|--------------|-----------|--------------|----------------|
| **Baseline** | Ваша базовая модель | X.XM | XX.X% | XX.X% |
| **3.1-A** | 32→64→128→256 | X.XM | XX.X% | XX.X% |
| **3.1-B** | 64→128→256 | X.XM | XX.X% | XX.X% |
| **3.2-A** | [1,1,1,1] блоков | X.XM | XX.X% | XX.X% |
| **3.2-B** | [2,2,2,2] блоков | X.XM | XX.X% | XX.X% |
| **3.2-C** | [3,3,3,3] блоков | X.XM | XX.X% | XX.X% |
| **3.3-A** | ReLU | X.XM | XX.X% | XX.X% |
| **3.3-B** | LeakyReLU | X.XM | XX.X% | XX.X% |
| **3.3-C** | ELU | X.XM | XX.X% | XX.X% |
| **3.3-D** | GELU | X.XM | XX.X% | XX.X% |
| **Final** | Лучшая конфигурация | X.XM | XX.X% | XX.X% |

**Анализ:**
- Какая конфигурация показала лучший результат?
- Есть ли признаки переобучения (большая разница между train и val)?
