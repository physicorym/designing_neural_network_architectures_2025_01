# Домашнее задание 2: Создание и оптимизация ResNet18

---

## Общая цель

Поэтапная разработка кастомной ResNet18 модели для классификации Tiny ImageNet с анализом влияния различных архитектурных решений на производительность.

**Датасет:** Tiny ImageNet (200 классов) - выберите **10 классов** самостоятельно для работы

---

## Часть 1: Подготовка данных

### Создание датакласса

Реализуйте собственный класс `TinyImageNetDataset`, наследующий от `torch.utils.data.Dataset`:

- Метод `__init__`: инициализация путей к данным, загрузка списка изображений и меток
- Метод `__len__`: возврат количества примеров в датасете
- Метод `__getitem__`: загрузка и возврат одного примера (изображение + метка)

## Часть 2: Базовая архитектура ResNet18

### Реализация Basic Block

Создайте базовый residual блок со следующей структурой:

```
Input
  ↓
Conv2d(kernel_size=3, padding=1, stride=stride)
  ↓
BatchNorm2d
  ↓
ReLU
  ↓
Conv2d(kernel_size=3, padding=1, stride=1)
  ↓
BatchNorm2d
  ↓
  + ← Skip Connection (с возможностью downsample)
  ↓
ReLU
  ↓
Output
```

**Важно:**
- Если входные и выходные размерности не совпадают, используйте skip connection с Conv2d(1x1) + BatchNorm2d
- Первый residual блок может иметь stride=2 для уменьшения пространственного размера

### Реализация ResNet18

Создайте архитектуру ResNet18 со следующей структурой:

```
Input (3, 64, 64)
  ↓
Conv2d(3→64, kernel_size=7, stride=2, padding=3)  # или 3x3, stride=1 для Tiny ImageNet
  ↓
BatchNorm2d
  ↓
ReLU
  ↓
MaxPool2d(kernel_size=3, stride=2, padding=1)  # опционально для Tiny ImageNet
  ↓
Layer1: 2x Basic Block (64 channels)
  ↓
Layer2: 2x Basic Block (128 channels, stride=2 в первом блоке)
  ↓
Layer3: 2x Basic Block (256 channels, stride=2 в первом блоке)
  ↓
Layer4: 2x Basic Block (512 channels, stride=2 в первом блоке)  # ОПЦИОНАЛЬНО
  ↓
AdaptiveAvgPool2d(output_size=(1, 1))
  ↓
Flatten
  ↓
Linear(512 → 10)  # 10 классов
  ↓
Output
```

### Ограничения для базовой модели:

- **Общее количество параметров:** не более **5 миллионов**
- **Количество residual блоков:** не более **8 блоков** (4 слоя по 2 блока)
- **Максимальное количество каналов:** до **512**

### Скрипт обучения

Реализуйте цикл обучения с следующими компонентами:

**Оптимизатор:**
- Adam или SGD с momentum=0.9
- Learning rate: 0.001

**Loss function:**
- CrossEntropyLoss

**Метрики:**
- Accuracy для train/validation
- Loss для train/validation

**Обучение:**
- Количество эпох: 20-30
- Логирование метрик на каждой эпохе

### Задача 2.4: Визуализация базовых результатов

После обучения базовой модели создайте:

1. **График Accuracy:**
   - X: эпохи
   - Y: accuracy
   - Две линии: train и validation

2. **График Loss:**
   - X: эпохи
   - Y: loss
   - Две линии: train и validation

3. **Информация о модели:**
   - Общее количество параметров
   - Архитектура (выведите через print(model))


## Часть 3: Поэтапная оптимизация модели

### Этап 3.1: Оптимизация количества каналов

**Цель:** Изучение влияния количества каналов на производительность.

**Эксперимент:**
- Создайте 2 варианта модели:
  - **Вариант A:** 32 → 64 → 128 → 256 каналов
  - **Вариант B:** 64 → 128 → 256 каналов (без 4-го слоя)
- Обучите обе модели с теми же гиперпараметрами
- Сравните:
  - Количество параметров
  - Validation accuracy

**Результат:**
- Таблица сравнения
- Графики accuracy и loss для обоих вариантов
- Вывод: какая конфигурация лучше?

### Этап 3.2: Эксперименты с функциями активации

**Цель:** Исследование влияния различных активаций на обучение.

**Модификация модели:**
Замените ReLU на другие функции активации:

**Эксперимент:**
- Используйте лучшую модель из Этапа 3.2
- Обучите модели с разными активациями:
  - **Вариант A:** ReLU (baseline)
  - **Вариант B:** LeakyReLU(negative_slope=0.01)
  - **Вариант C:** ELU(alpha=1.0)
  - **Вариант D:** GELU

**Важно:** Используйте `inplace=True` где возможно для экономии памяти

**Результат:**
- Сравнение скорости сходимости (accuracy на каждой эпохе)
- Финальная validation accuracy
- Время обучения
- Вывод: какая активация работает лучше?


## Часть 4: Финальная модель и тестирование

### Задача 4.1: Создание финальной модели

На основе всех экспериментов:
- Выберите лучшую конфигурацию каналов
- Выберите по Вашему мнению лучшее количество слоев (объясните почему именно такое количество)
- Выберите лучшую функцию активации
- Обучите финальную модель на 30-40 эпох

**Дополнительно (опционально):**
- Добавьте data augmentation
- Попробуйте другие оптимизаторы (AdamW как вариант)

### Задача 4.2: Тестирование на test set

После обучения финальной модели:

1. **Загрузите лучшую модель** (сохраненную по validation accuracy)
2. **Оцените на test set:**
   - Accuracy
   - Precision, Recall, F1-score для каждого класса
   - Confusion Matrix

### Задача 4.3: Визуальный анализ

Создайте визуализацию с 10 случайными примерами из test set:

```
[Изображение 1] | Истинный класс: cat    | Предсказание: cat
[Изображение 2] | Истинный класс: dog    | Предсказание: wolf
...
```